{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, \"b c h w -> b h w c\")\n",
    "        x = super().forward(x)\n",
    "        x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class OverlapPatchMerging(nn.Sequential):\n",
    "    # input image with tensor b, c, h, w\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels:int, out_channels:int, patch_size:int, overlap_size:int, \n",
    "    ):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=patch_size,\n",
    "                stride=overlap_size,\n",
    "                padding=patch_size//2,\n",
    "                bias=False\n",
    "            ),\n",
    "            # Layer Norm\n",
    "            LayerNorm2d(out_channels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 768, 128, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N, C, H, W = 5, 3, 512, 512\n",
    "testinput = torch.randn(N, C, H, W)\n",
    "merge_encode_768 = OverlapPatchMerging(3,768,7,4)\n",
    "out_test = merge_encode_768(testinput)\n",
    "out_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import einsum\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, *, dim, heads, reduction_ratio):\n",
    "        super(EfficientSelfAttention, self).__init__()\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.heads = heads\n",
    "\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim * 2, reduction_ratio, stride = reduction_ratio, bias = False)\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim=1))\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h=heads), (q, k, v))\n",
    "\n",
    "        # q @ k.transpose(-2, -1) * self.scale\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        # attn @ V\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h=heads, x=h, y=w)\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 768, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "heads = 2      # Number of attention heads\n",
    "reduction_ratio = 4  # Reduction ratio for keys and values\n",
    "\n",
    "efficient_attn = EfficientSelfAttention(dim=768, heads=heads, reduction_ratio=reduction_ratio)\n",
    "output = efficient_attn(out_test)\n",
    "print(output.shape)  # Should print the output shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixMLP(nn.Sequential):\n",
    "    def __init__(self, channels: int, expansion: int = 4):\n",
    "        super().__init__(\n",
    "            # dense layer\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),\n",
    "            # depth wise conv\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels * expansion,\n",
    "                kernel_size=3,\n",
    "                groups=channels,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            # dense layer\n",
    "            nn.Conv2d(channels * expansion, channels, kernel_size=1),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
